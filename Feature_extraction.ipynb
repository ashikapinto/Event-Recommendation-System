{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import _pickle as cPickle \n",
    "import scipy.sparse as ss\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "user_index = cPickle.load(open(\"user_index.pkl\", 'rb'))\n",
    "event_index = cPickle.load(open(\"event_index.pkl\", 'rb'))\n",
    "user_event_scores = sio.mmread(\"user_event_response\").todense()\n",
    "user_sim_matrix = sio.mmread(\"similaritymatrix\").todense()\n",
    "event_prop_sim = sio.mmread(\"EV_eventPropSim\").todense()\n",
    "event_cont_sim = sio.mmread(\"EV_eventContSim\").todense()\n",
    "num_friends = sio.mmread(\"num_friends\")\n",
    "user_friends = sio.mmread(\"user_friends\").todense()\n",
    "event_popularity = sio.mmread(\"EA_eventPopularity\").todense()\n",
    "#print(user_event_scores)   \n",
    "#FEATURE EXTRACTION:\n",
    "#Now that we have stored all our data in a desirable format, we should extract our new features based on \n",
    "#various information collected so far and use these features in our final model.\n",
    "\n",
    "#We use the following function to take each entry from our training dataset and convert it into a set of new features \n",
    "#and store it in a new dataset.We will later use this set of newly formed features as input to our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import _pickle as cPickle\n",
    "import scipy.io\n",
    "import scipy.sparse as ss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def userReco(user_id, event_id):\n",
    "    \"\"\"\n",
    "    Collaborative filtering:-\n",
    "    for item i\n",
    "      for every other user v that has a preference for i\n",
    "        compute similarity s between u and v\n",
    "        incorporate v's preference for i weighted by s into running aversge\n",
    "    return top items ranked by weighted average\n",
    "    1. Get the colunm of scores given by all the users to the given event.\n",
    "    2. Get the similarity(row matrix) of the user in query to all the users in the system(train and test dataset).\n",
    "    3. Multipy the two matrices obtained to find out the user reocommender score for the event in query based on\n",
    "    the preferences of all the users similar to the user in query.  \n",
    "    \"\"\"\n",
    "    i = user_index[user_id]\n",
    "    j = event_index[event_id]\n",
    "    vs = user_event_scores[:, j]\n",
    "    sims = user_sim_matrix[i, :]\n",
    "    user_pref_score = sims * vs\n",
    "    try:\n",
    "        return user_pref_score[0, 0] - user_event_scores[i, j]\n",
    "    except IndexError:\n",
    "        return 0\n",
    "\n",
    "def eventReco(user_id, event_id):\n",
    "    \"\"\"\n",
    "    Content based:-\n",
    "    for item i\n",
    "      for every item j tht u has a preference for\n",
    "        compute similarity s between i and j\n",
    "        add u's preference for j weighted by s to a running average\n",
    "    return top items, ranked by weighted average\n",
    "    1. Get the row with all the event scores given by a ith user to all the event.\n",
    "    2. Get the columns from the event-event similarity matrix based on metadata and and content.\n",
    "    3. Multiply the row matrix in step 1 and column matrix in step two for the meta data based and content based seperately\n",
    "    to obtain the event recommender score based on metadata and content.\n",
    "    \"\"\"\n",
    "    i = user_index[user_id]\n",
    "    j = event_index[event_id]\n",
    "    js = user_event_scores[i, :]\n",
    "    psim = event_prop_sim[:, j]\n",
    "    csim = event_cont_sim[:, j]\n",
    "    pprod = js * psim\n",
    "    cprod = js * csim\n",
    "    pscore = 0\n",
    "    cscore = 0\n",
    "    #print(cprod)\n",
    "    try:\n",
    "        pscore = pprod[0, 0] - user_event_scores[i, j]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        cscore = cprod[0, 0] - user_event_scores[i, j]\n",
    "        #print(cscore)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    #print(pscore,cscore,user_id)\n",
    "    \n",
    "    return pscore, cscore\n",
    "\n",
    "\n",
    "def userPop(user_id):\n",
    "    \"\"\"\n",
    "    We use the matrix the matrix that was previously genrated to get the number of friends the user has.\n",
    "    Measures user popularity by number of friends a user has. People\n",
    "    with more friends tend to be outgoing and are more likely to go\n",
    "    to events\n",
    "    \"\"\"\n",
    "    if user_id in user_index:\n",
    "        i = user_index[user_id]\n",
    "        try:\n",
    "            return num_friends[0, i]\n",
    "        except IndexError:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def friendInfluence(user_id):\n",
    "    \"\"\"\n",
    "    We use the user and friend influence matrix that was generated previously to get the influence measure\n",
    "    of the friend on the user in query.\n",
    "    Measures friends influence by the friends who are known (from the\n",
    "    training set) to go or not go to an event. The average of scores across\n",
    "    all friends of the user is the influence score.\n",
    "    \"\"\"\n",
    "    nusers = np.shape(user_friends)[1]\n",
    "    i = user_index[user_id]\n",
    "    return (user_friends[i, :].sum(axis=0) /nusers)[0,0]\n",
    "\n",
    "def eventPop(event_id):\n",
    "    \"\"\"\n",
    "    Measures event popularity by the number attending, maybe(This number of the event record also depicts the interest of\n",
    "     the user in the given query) and not attending.\n",
    "    \"\"\"\n",
    "    i = event_index[event_id]\n",
    "    return event_popularity[i, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewriting training data...\n",
      "rewriting test data...\n"
     ]
    }
   ],
   "source": [
    "def getfeatures(start=1, train=True, head=True):\n",
    "    if(train):\n",
    "        file = \"train.csv\" \n",
    "    else:\n",
    "        file=\"test.csv\"\n",
    "    #the input train is used to indicate whether it is a train or test data set.Feature extraction is done for both.\n",
    "    fin = open(file, 'r')\n",
    "    fout = open( \"newdata/\"+file, 'w')\n",
    "    # the above code creates file descriptors for reading the dataset and writing the features into a new dataset\n",
    "    if head:\n",
    "        columns = [\"invited\", \"user_reco\", \"evt_p_reco\",\"evt_c_reco\", \"user_pop\", \"frnd_infl\", \"evt_pop\"]\n",
    "        if train:\n",
    "            columns.append(\"interested\")\n",
    "            columns.append(\"not_interested\")\n",
    "        #because  the interested and not interested features are only present in our training set\n",
    "        fout.write(\",\".join(columns) + \"\\n\")\n",
    "    #write these column names into the new data file and seperate them with a comma\n",
    "    r = 0\n",
    "    #for every line in the dataset : calculate new features and add to new dataset\n",
    "    for each_line in fin:\n",
    "        r += 1\n",
    "        if r < start:\n",
    "            continue\n",
    "        #skip the first line\n",
    "        cols = each_line.strip().split(\",\")\n",
    "        user_id = cols[0]\n",
    "        event_id = cols[1]\n",
    "        invited = cols[2]\n",
    "        #get the above attributes from dataset and use them along with the data stored in matrices to calculate features.\n",
    "        user_reco = userReco(user_id, event_id)\n",
    "        evt_p_reco, evt_c_reco = eventReco(user_id, event_id)\n",
    "        user_pop = userPop(user_id)\n",
    "        frnd_infl = friendInfluence(user_id)\n",
    "        evt_pop = eventPop(event_id)\n",
    "        new_columns = [invited, user_reco, evt_p_reco,evt_c_reco, user_pop, frnd_infl, evt_pop]\n",
    "        if train:\n",
    "            new_columns.append(cols[4]) # interested\n",
    "            new_columns.append(cols[5]) # not_interested\n",
    "        fout.write(\",\".join(map(lambda x: str(x), new_columns)) + \"\\n\")\n",
    "    fin.close()\n",
    "    fout.close()\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"rewriting training data...\")\n",
    "getfeatures(train=True, start=2, head=True)\n",
    "print (\"rewriting test data...\")\n",
    "getfeatures(train=False, start=2, head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
